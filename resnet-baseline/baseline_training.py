#!/usr/bin/env python3
"""
åŸºçº¿æ¨¡å‹è®­ç»ƒè„šæœ¬ - RTX 4090ä¼˜åŒ–ç‰ˆ
ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒå’Œä¼˜åŒ–çš„é…ç½®
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import DataLoader
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
from tqdm import tqdm
import os
import json
import time
import random
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æ¨¡å‹
from baseline_models import (
    VanillaResNet18, 
    ResNet18FocalLoss,
    ResNet18WithSkip, 
    ImprovedResNet18Full,
    FocalLoss,
    ModelConfig
)
from simplified_cbas_dataset import create_dataloaders

# å¯¼å…¥RTX 4090ä¼˜åŒ–é…ç½®
from experiment_config_4090 import (
    ExperimentConfig, 
    AMPTrainer, 
    GPUMonitor,
    WarmupCosineScheduler
)


# ==================================
# éšæœºç§å­è®¾ç½®
# ==================================
def set_random_seeds(seed=42, deterministic=True):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    else:
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True
    
    print(f"âœ“ éšæœºç§å­è®¾ç½®å®Œæˆ: seed={seed}, deterministic={deterministic}")


# ==================================
# æ—©åœæœºåˆ¶
# ==================================
class EarlyStopping:
    """æ”¹è¿›çš„æ—©åœæœºåˆ¶ï¼Œæ”¯æŒæ›´ç»†ç²’åº¦çš„æ§åˆ¶"""
    
    def __init__(self, patience=10, min_delta=0.001, mode='max', 
                 metric_name='metric', verbose=True):
        """
        Args:
            patience: å¤šå°‘æ¬¡éªŒè¯æ— æ”¹å–„ååœæ­¢
            min_delta: æœ€å°æ”¹å–„é˜ˆå€¼
            mode: 'max'è¡¨ç¤ºæŒ‡æ ‡è¶Šå¤§è¶Šå¥½ï¼Œ'min'è¡¨ç¤ºè¶Šå°è¶Šå¥½
            metric_name: ç›‘æ§çš„æŒ‡æ ‡åç§°ï¼ˆç”¨äºæ‰“å°ï¼‰
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.metric_name = metric_name
        self.verbose = verbose
        
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_epoch = 0
        self.history = []  # è®°å½•å†å²åˆ†æ•°
        
        # è®¾ç½®æ¯”è¾ƒå‡½æ•°
        if mode == 'max':
            self.monitor_op = lambda current, best: current > best + min_delta
            self.best_score_init = -float('inf')
        else:
            self.monitor_op = lambda current, best: current < best - min_delta
            self.best_score_init = float('inf')
    
    def __call__(self, score, epoch):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ"""
        self.history.append(score)
        
        if self.best_score is None:
            self.best_score = score
            self.best_epoch = epoch
            if self.verbose:
                print(f'  âœ“ æ—©åœç›‘æ§åˆå§‹åŒ– - {self.metric_name}: {score:.4f}')
        elif self.monitor_op(score, self.best_score):
            improvement = score - self.best_score if self.mode == 'max' else self.best_score - score
            self.best_score = score
            self.best_epoch = epoch
            self.counter = 0
            if self.verbose:
                print(f'  âœ“ æ€§èƒ½æå‡ï¼{self.metric_name}: {score:.4f} (â†‘{improvement:.4f})')
        else:
            self.counter += 1
            if self.verbose and self.counter > 0 and self.counter % 3 == 0:  # æ¯3æ¬¡æ‰“å°ä¸€æ¬¡
                print(f'  âš  æ—©åœè®¡æ•°: {self.counter}/{self.patience} (æœ€ä½³: {self.best_score:.4f} @ Epoch {self.best_epoch})')
            
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print(f'\n  ğŸ›‘ è§¦å‘æ—©åœï¼')
                    print(f'     æœ€ä½³{self.metric_name}: {self.best_score:.4f} @ Epoch {self.best_epoch}')
                    print(f'     å·²ç» {self.patience} æ¬¡éªŒè¯æ— æ”¹å–„\n')
        
        return self.early_stop
    
    def get_stats(self):
        """è·å–æ—©åœç»Ÿè®¡ä¿¡æ¯"""
        return {
            'best_score': self.best_score,
            'best_epoch': self.best_epoch,
            'stopped_early': self.early_stop,
            'patience_used': self.counter,
            'total_validations': len(self.history),
            'score_history': self.history
        }
    
    def reset(self):
        """é‡ç½®æ—©åœçŠ¶æ€"""
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_epoch = 0
        self.history = []


# ==================================
# ç»¼åˆè¯„ä¼°æŒ‡æ ‡
# ==================================
class ComprehensiveMetrics:
    """è®ºæ–‡éœ€è¦çš„å…¨é¢è¯„ä¼°æŒ‡æ ‡"""
    
    def __init__(self, num_classes=2):
        self.num_classes = num_classes
        self.reset()
        
    def reset(self):
        """é‡ç½®æ‰€æœ‰æŒ‡æ ‡"""
        self.confusion_matrix = np.zeros((self.num_classes, self.num_classes))
        self.tp = np.zeros(self.num_classes)
        self.fp = np.zeros(self.num_classes)
        self.fn = np.zeros(self.num_classes)
        self.tn = np.zeros(self.num_classes)
        self.total_pixels = 0
        
        # ç”¨äºè¾¹ç•Œè¯„ä¼°
        self.boundary_tp = 0
        self.boundary_fp = 0
        self.boundary_fn = 0
        
        # ç”¨äºæ•ˆç‡è¯„ä¼°
        self.inference_times = []
        
    def update(self, pred, target):
        """æ›´æ–°æŒ‡æ ‡"""
        # ç¡®ä¿æ˜¯numpyæ•°ç»„
        if torch.is_tensor(pred):
            pred = pred.cpu().numpy()
        if torch.is_tensor(target):
            target = target.cpu().numpy()
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(target.flatten(), pred.flatten(), labels=list(range(self.num_classes)))
        self.confusion_matrix += cm
        
        # åˆ†ç±»ç»Ÿè®¡
        for c in range(self.num_classes):
            pred_c = (pred == c)
            target_c = (target == c)
            
            self.tp[c] += np.sum(pred_c & target_c)
            self.fp[c] += np.sum(pred_c & ~target_c)
            self.fn[c] += np.sum(~pred_c & target_c)
            self.tn[c] += np.sum(~pred_c & ~target_c)
            
        self.total_pixels += pred.size
        
    def add_inference_time(self, time_ms):
        """æ·»åŠ æ¨ç†æ—¶é—´"""
        self.inference_times.append(time_ms)
        
    def compute_metrics(self):
        """è®¡ç®—æ‰€æœ‰æŒ‡æ ‡"""
        metrics = {}
        eps = 1e-10  # é˜²æ­¢é™¤é›¶
        
        # åƒç´ çº§å‡†ç¡®ç‡
        metrics['overall_accuracy'] = np.trace(self.confusion_matrix) / (np.sum(self.confusion_matrix) + eps)
        
        # æ¯ç±»IoU
        for c in range(self.num_classes):
            intersection = self.tp[c]
            union = self.tp[c] + self.fp[c] + self.fn[c]
            iou = intersection / (union + eps)
            metrics[f'iou_class_{c}'] = iou
            
        # mIoU
        metrics['mIoU'] = np.mean([metrics[f'iou_class_{i}'] for i in range(self.num_classes)])
        
        # è¿‡ç«åŒºIoUï¼ˆä¸»è¦æŒ‡æ ‡ï¼‰
        metrics['burned_area_iou'] = metrics['iou_class_1']
        
        # ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1
        for c in range(self.num_classes):
            precision = self.tp[c] / (self.tp[c] + self.fp[c] + eps)
            recall = self.tp[c] / (self.tp[c] + self.fn[c] + eps)
            f1 = 2 * precision * recall / (precision + recall + eps)
            
            metrics[f'precision_class_{c}'] = precision
            metrics[f'recall_class_{c}'] = recall
            metrics[f'f1_class_{c}'] = f1
            
        # è¿‡ç«åŒºæŒ‡æ ‡
        metrics['burned_precision'] = metrics['precision_class_1']
        metrics['burned_recall'] = metrics['recall_class_1']
        metrics['burned_f1'] = metrics['f1_class_1']
        
        # æ•ˆç‡æŒ‡æ ‡
        if self.inference_times:
            metrics['avg_inference_time_ms'] = np.mean(self.inference_times)
            metrics['std_inference_time_ms'] = np.std(self.inference_times)
            metrics['fps'] = 1000.0 / (metrics['avg_inference_time_ms'] + eps)
            
        # Kappaç³»æ•°
        po = metrics['overall_accuracy']
        pe = np.sum(self.confusion_matrix.sum(axis=0) * self.confusion_matrix.sum(axis=1)) / ((self.total_pixels ** 2) + eps)
        metrics['kappa'] = (po - pe) / (1 - pe + eps)
        
        # ç±»åˆ«åˆ†å¸ƒ
        for c in range(self.num_classes):
            metrics[f'class_{c}_ratio'] = self.confusion_matrix.sum(axis=0)[c] / (self.total_pixels + eps)
            
        return metrics


# ==================================
# æŸå¤±å‡½æ•°å·¥å‚
# ==================================
def get_loss_function(config, device):
    """æ ¹æ®é…ç½®è·å–æŸå¤±å‡½æ•°"""
    loss_type = config['loss_type']
    loss_params = config.get('loss_params', {})
    
    if loss_type == 'CrossEntropy':
        return nn.CrossEntropyLoss()
    elif loss_type == 'FocalLoss':
        alpha = loss_params.get('alpha', None)
        gamma = loss_params.get('gamma', 2.0)
        if alpha is not None:
            alpha = torch.tensor(alpha).to(device)
        return FocalLoss(alpha=alpha, gamma=gamma)
    else:
        raise ValueError(f"Unknown loss type: {loss_type}")


# ==================================
# è®­ç»ƒå‡½æ•° - RTX 4090ä¼˜åŒ–ç‰ˆ
# ==================================
def train_baseline_model(model, config, train_loader, val_loader):
    """è®­ç»ƒå•ä¸ªåŸºçº¿æ¨¡å‹ - æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ"""
    
    model_name = config['model_name']
    print(f"\n{'='*70}")
    print(f"è®­ç»ƒæ¨¡å‹: {model_name}")
    print(f"é…ç½®ä¿¡æ¯:")
    print(f"  - å­¦ä¹ ç‡: {config.get('scaled_learning_rate', config['learning_rate']):.4f}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    print(f"  - è®­ç»ƒè½®æ•°: {config['epochs']}")
    print(f"  - æŸå¤±å‡½æ•°: {config['loss_type']}")
    print(f"  - æ··åˆç²¾åº¦: {'å¯ç”¨' if config.get('use_amp', False) else 'ç¦ç”¨'}")
    print(f"  - éªŒè¯é¢‘ç‡: æ¯{config['validate_every']}ä¸ªepoch")
    print(f"  - æ—©åœ: {'å¯ç”¨' if config['use_early_stopping'] else 'ç¦ç”¨'}")
    if config['use_early_stopping']:
        print(f"    - è€å¿ƒ: {config['early_stopping_patience']}æ¬¡éªŒè¯")
        print(f"    - æœ€å°æ”¹å–„: {config['early_stopping_min_delta']}")
    print(f"  - éšæœºç§å­: {config['seed']}")
    print(f"{'='*70}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # GPUå†…å­˜ç›‘æ§
    if config.get('track_gpu_memory', False) and torch.cuda.is_available():
        GPUMonitor.print_gpu_utilization()
    
    # æŸå¤±å‡½æ•°
    criterion = get_loss_function(config, device)
    print(f"âœ“ æŸå¤±å‡½æ•°: {config['loss_type']}")
    if config['loss_type'] == 'FocalLoss':
        print(f"  - Alpha: {config['loss_params'].get('alpha', 'None')}")
        print(f"  - Gamma: {config['loss_params'].get('gamma', 2.0)}")
    
    # ä¼˜åŒ–å™¨
    optimizer = AdamW(
        model.parameters(), 
        lr=config.get('scaled_learning_rate', config['learning_rate']), 
        weight_decay=config['weight_decay']
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if config.get('warmup_epochs', 0) > 0:
        # ä½¿ç”¨å¸¦é¢„çƒ­çš„è°ƒåº¦å™¨
        scheduler = WarmupCosineScheduler(
            optimizer,
            warmup_epochs=config['warmup_epochs'],
            total_epochs=config['epochs'],
            eta_min=config['scheduler_eta_min']
        )
        print(f"âœ“ å­¦ä¹ ç‡è°ƒåº¦: é¢„çƒ­{config['warmup_epochs']}è½® + ä½™å¼¦é€€ç«")
    else:
        # æ ‡å‡†ä½™å¼¦é€€ç«
        scheduler = CosineAnnealingLR(
            optimizer, 
            T_max=config['scheduler_T_max'], 
            eta_min=config['scheduler_eta_min']
        )
        print(f"âœ“ å­¦ä¹ ç‡è°ƒåº¦: ä½™å¼¦é€€ç«")
    
    # åˆå§‹åŒ–AMPè®­ç»ƒå™¨
    amp_trainer = AMPTrainer(model, config)
    
    # åˆå§‹åŒ–æ—©åœæœºåˆ¶
    early_stopping = None
    if config['use_early_stopping']:
        early_stopping = EarlyStopping(
            patience=config['early_stopping_patience'],
            min_delta=config['early_stopping_min_delta'],
            mode=config['early_stopping_mode'],
            metric_name=config['early_stopping_metric'],
            verbose=config['early_stopping_verbose']
        )
        print(f"âœ“ æ—©åœæœºåˆ¶å·²å¯ç”¨\n")
    
    # è®­ç»ƒå†å²
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_metrics': [],
        'learning_rates': [],
        'epochs_trained': 0,
        'early_stopping_stats': None
    }
    
    # æœ€ä½³æ¨¡å‹è·Ÿè¸ª
    best_iou = 0.0
    best_epoch = 0
    actual_epochs = 0
    
    # è¯„ä¼°å™¨
    metrics_evaluator = ComprehensiveMetrics()
    
    # æ¢¯åº¦ç´¯ç§¯è®¾ç½®
    accumulation_steps = config.get('gradient_accumulation_steps', 1)
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(config['epochs']):
        actual_epochs = epoch + 1
        epoch_start_time = time.time()
        
        # ============ è®­ç»ƒé˜¶æ®µ ============
        model.train()
        train_loss = 0.0
        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config["epochs"]} [Train]')
        
        optimizer.zero_grad()
        
        for batch_idx, (images, masks) in enumerate(train_bar):
            images = images.to(device)
            masks = masks.to(device)
            
            # ä½¿ç”¨AMPTrainerè¿›è¡Œè®­ç»ƒæ­¥éª¤ï¼ˆæ”¯æŒæ··åˆç²¾åº¦ï¼‰
            if accumulation_steps > 1:
                # æ¢¯åº¦ç´¯ç§¯
                loss_scale = 1.0 / accumulation_steps
                
                # è®¡ç®—æŸå¤±å’Œåå‘ä¼ æ’­
                if config.get('use_amp', False):
                    with torch.cuda.amp.autocast():
                        outputs = model(images)
                        loss = criterion(outputs, masks) * loss_scale
                    
                    # ç¼©æ”¾æ¢¯åº¦å¹¶åå‘ä¼ æ’­
                    amp_trainer.scaler.scale(loss).backward()
                else:
                    outputs = model(images)
                    loss = criterion(outputs, masks) * loss_scale
                    loss.backward()
                
                train_loss += loss.item() * accumulation_steps  # æ¢å¤åŸå§‹æŸå¤±å€¼
                
                # ç´¯ç§¯æ­¥æ•°è¾¾åˆ°æ—¶æ›´æ–°æƒé‡
                if (batch_idx + 1) % accumulation_steps == 0:
                    if config.get('use_amp', False):
                        # å…ˆunscaleæ¢¯åº¦
                        amp_trainer.scaler.unscale_(optimizer)
                        
                        # æ¢¯åº¦è£å‰ª
                        if config.get('grad_clip', 0) > 0:
                            torch.nn.utils.clip_grad_norm_(
                                model.parameters(), 
                                config['grad_clip']
                            )
                        
                        # æ›´æ–°æƒé‡
                        amp_trainer.scaler.step(optimizer)
                        amp_trainer.scaler.update()
                    else:
                        # éAMPæƒ…å†µ
                        if config.get('grad_clip', 0) > 0:
                            torch.nn.utils.clip_grad_norm_(
                                model.parameters(), 
                                config['grad_clip']
                            )
                        optimizer.step()
                    
                    optimizer.zero_grad()
            else:
                # æ— æ¢¯åº¦ç´¯ç§¯çš„æ ‡å‡†è®­ç»ƒ
                loss_value, outputs = amp_trainer.train_step(
                    images, masks, criterion, optimizer
                )
                train_loss += loss_value
            
            # å®šæœŸæ‰“å°
            if batch_idx % config['log_interval'] == 0:
                current_lr = scheduler.get_last_lr()[0] if hasattr(scheduler, 'get_last_lr') else optimizer.param_groups[0]['lr']
                train_bar.set_postfix({
                    'loss': f'{loss_value:.4f}',
                    'lr': f'{current_lr:.6f}'
                })
        
        avg_train_loss = train_loss / len(train_loader)
        history['train_loss'].append(avg_train_loss)
        current_lr = scheduler.get_last_lr()[0] if hasattr(scheduler, 'get_last_lr') else optimizer.param_groups[0]['lr']
        history['learning_rates'].append(current_lr)
        
        # å­¦ä¹ ç‡è°ƒæ•´
        scheduler.step()
        
        # ============ éªŒè¯é˜¶æ®µ ============
        if (epoch + 1) % config['validate_every'] == 0:
            model.eval()
            val_loss = 0.0
            metrics_evaluator.reset()
            
            with torch.no_grad():
                val_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{config["epochs"]} [Val]')
                
                # æ··åˆç²¾åº¦éªŒè¯
                use_amp_val = config.get('use_amp', False) and torch.cuda.is_available()
                
                for images, masks in val_bar:
                    images = images.to(device)
                    masks = masks.to(device)
                    
                    # æµ‹é‡æ¨ç†æ—¶é—´ï¼ˆæœ€åå‡ ä¸ªepochï¼‰
                    if epoch >= config['epochs'] - 5:
                        torch.cuda.synchronize() if torch.cuda.is_available() else None
                        start_time = time.time()
                    
                    # å‰å‘ä¼ æ’­ï¼ˆæ”¯æŒæ··åˆç²¾åº¦ï¼‰
                    if use_amp_val:
                        with torch.cuda.amp.autocast():
                            outputs = model(images)
                    else:
                        outputs = model(images)
                    
                    if epoch >= config['epochs'] - 5:
                        torch.cuda.synchronize() if torch.cuda.is_available() else None
                        inference_time = (time.time() - start_time) * 1000
                        metrics_evaluator.add_inference_time(inference_time)
                    
                    # å¤„ç†è¾“å‡º
                    if isinstance(outputs, tuple):
                        outputs = outputs[0]
                    
                    # è®¡ç®—æŸå¤±
                    loss = criterion(outputs, masks)
                    

                    # è°ƒè¯•NaNé—®é¢˜
                    if torch.isnan(loss):
                        print(f"\næ£€æµ‹åˆ°NaNæŸå¤±ï¼è¿›è¡Œè°ƒè¯•...")
                        print(f"1. è¾“å‡ºç»Ÿè®¡:")
                        print(f"   - æœ€å°å€¼: {outputs.min():.4f}")
                        print(f"   - æœ€å¤§å€¼: {outputs.max():.4f}")
                        print(f"   - åŒ…å«inf: {torch.isinf(outputs).any()}")
                        print(f"   - åŒ…å«nan: {torch.isnan(outputs).any()}")
                        
                        print(f"2. æ ‡ç­¾ç»Ÿè®¡:")
                        print(f"   - å”¯ä¸€å€¼: {torch.unique(masks).tolist()}")
                        print(f"   - ç±»åˆ«0æ•°é‡: {(masks == 0).sum().item()}")
                        print(f"   - ç±»åˆ«1æ•°é‡: {(masks == 1).sum().item()}")
                        
                        print(f"3. é¢„æµ‹ç»Ÿè®¡:")
                        preds = outputs.argmax(dim=1)
                        print(f"   - é¢„æµ‹ç±»åˆ«0: {(preds == 0).sum().item()}")
                        print(f"   - é¢„æµ‹ç±»åˆ«1: {(preds == 1).sum().item()}")
                        
                        # æ£€æŸ¥æ¦‚ç‡
                        probs = torch.softmax(outputs, dim=1)
                        print(f"4. æ¦‚ç‡ç»Ÿè®¡:")
                        print(f"   - æœ€å°æ¦‚ç‡: {probs.min():.6f}")
                        print(f"   - æœ€å¤§æ¦‚ç‡: {probs.max():.6f}")
                        
                        # å°è¯•ä½¿ç”¨æ ‡å‡†CEæŸå¤±
                        try:
                            ce_test = nn.CrossEntropyLoss()(outputs, masks)
                            print(f"5. æ ‡å‡†CrossEntropyæŸå¤±: {ce_test.item():.4f}")
                            if torch.isnan(ce_test):
                                print("   æ ‡å‡†CEä¹Ÿæ˜¯NaN - é—®é¢˜åœ¨æ¨¡å‹è¾“å‡º")
                            else:
                                print("   æ ‡å‡†CEæ­£å¸¸ - é—®é¢˜åœ¨FocalLosså®ç°")
                        except Exception as e:
                            print(f"   è®¡ç®—æ ‡å‡†CEå¤±è´¥: {e}")
                        
                        # ä½¿ç”¨0æ›¿ä»£nanç»§ç»­è®­ç»ƒ
                        loss = torch.tensor(0.0, device=loss.device)
                        print("ä½¿ç”¨0æ›¿ä»£NaNæŸå¤±ç»§ç»­...")

                    
                    val_loss += loss.item()
                    
                    # é¢„æµ‹
                    preds = outputs.argmax(dim=1)
                    
                    # æ›´æ–°æŒ‡æ ‡
                    metrics_evaluator.update(
                        preds.cpu().numpy(),
                        masks.cpu().numpy()
                    )
            
            # è®¡ç®—æŒ‡æ ‡
            avg_val_loss = val_loss / len(val_loader)
            metrics = metrics_evaluator.compute_metrics()
            
            history['val_loss'].append(avg_val_loss)
            history['val_metrics'].append(metrics)
            
            # è®¡ç®—epochæ—¶é—´
            epoch_time = time.time() - epoch_start_time
            
            # æ‰“å°ç»“æœ
            print(f"\n{'='*70}")
            print(f"Epoch {epoch+1} éªŒè¯ç»“æœ (è€—æ—¶: {epoch_time:.1f}s):")
            print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
            print(f"  éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
            print(f"  å­¦ä¹ ç‡: {current_lr:.6f}")
            print(f"  æ•´ä½“å‡†ç¡®ç‡: {metrics['overall_accuracy']:.4f}")
            print(f"  è¿‡ç«åŒºIoU: {metrics['burned_area_iou']:.4f} (æœ€ä½³: {best_iou:.4f})")
            print(f"  å¹³å‡IoU: {metrics['mIoU']:.4f}")
            print(f"  è¿‡ç«åŒºç²¾ç¡®ç‡: {metrics['burned_precision']:.4f}")
            print(f"  è¿‡ç«åŒºå¬å›ç‡: {metrics['burned_recall']:.4f}")
            print(f"  è¿‡ç«åŒºF1: {metrics['burned_f1']:.4f}")
            if 'fps' in metrics:
                print(f"  æ¨ç†é€Ÿåº¦: {metrics['fps']:.1f} FPS")
            
            # GPUå†…å­˜ç›‘æ§
            if config.get('track_gpu_memory', False) and torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1024**3
                print(f"  GPUå†…å­˜ä½¿ç”¨: {allocated:.2f} GB")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if metrics['burned_area_iou'] > best_iou:
                best_iou = metrics['burned_area_iou']
                best_epoch = epoch + 1
                
                if config['save_best']:
                    save_path = os.path.join(config['save_dir'], f'{model_name}_best.pth')
                    torch.save({
                        'epoch': epoch + 1,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict() if hasattr(scheduler, 'state_dict') else None,
                        'best_iou': best_iou,
                        'metrics': metrics,
                        'config': config,
                        'history': history
                    }, save_path)
                    print(f'  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (IoU: {best_iou:.4f})')
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if config.get('save_checkpoint_every', 0) > 0 and (epoch + 1) % config['save_checkpoint_every'] == 0:
                checkpoint_path = os.path.join(config['save_dir'], f'{model_name}_checkpoint_epoch{epoch+1}.pth')
                torch.save({
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'current_iou': metrics['burned_area_iou'],
                    'best_iou': best_iou
                }, checkpoint_path)
                print(f'  âœ“ ä¿å­˜æ£€æŸ¥ç‚¹: epoch{epoch+1}')
            
            # æ—©åœæ£€æŸ¥
            if early_stopping is not None:
                stop = early_stopping(metrics[config['early_stopping_metric']], epoch + 1)
                if stop:
                    print(f"\n{'='*70}")
                    print(f"è®­ç»ƒå› æ—©åœæœºåˆ¶ç»ˆæ­¢äº Epoch {epoch + 1}")
                    print(f"æœ€ä½³æ€§èƒ½: {config['early_stopping_metric']}={early_stopping.best_score:.4f} @ Epoch {early_stopping.best_epoch}")
                    print(f"{'='*70}")
                    break
            
            print(f"{'='*70}\n")
    
    # è®°å½•æ—©åœç»Ÿè®¡
    if early_stopping is not None:
        history['early_stopping_stats'] = early_stopping.get_stats()
    
    history['epochs_trained'] = actual_epochs
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if config['save_last']:
        save_path = os.path.join(config['save_dir'], f'{model_name}_last.pth')
        torch.save({
            'epoch': actual_epochs,
            'model_state_dict': model.state_dict(),
            'best_iou': best_iou,
            'best_epoch': best_epoch,
            'final_metrics': history['val_metrics'][-1] if history['val_metrics'] else {},
            'config': config,
            'history': history,
            'early_stopping_stats': history['early_stopping_stats']
        }, save_path)
    
    return history, best_iou, best_epoch


# ==================================
# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
# ==================================
def generate_comparison_report(results_summary, save_dir):
    """ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Š"""
    
    print("\n" + "="*80)
    print("åŸºçº¿æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š - RTX 4090ä¼˜åŒ–ç‰ˆ")
    print("="*80)
    
    # è¡¨æ ¼å¤´
    headers = ['Model', 'Batch', 'Best IoU', 'Best Epoch', 'Trained', 'F1', 'Precision', 'Recall', 'FPS']
    col_widths = [20, 7, 10, 11, 9, 10, 10, 10, 10]
    
    # æ‰“å°è¡¨å¤´
    header_line = ""
    for header, width in zip(headers, col_widths):
        header_line += f"{header:<{width}}"
    print(f"\n{header_line}")
    print("-" * sum(col_widths))
    
    # æ‰“å°æ¯ä¸ªæ¨¡å‹çš„ç»“æœ
    for model_name, results in results_summary.items():
        metrics = results['final_metrics']
        epochs_trained = results.get('epochs_trained', 'N/A')
        batch_size = results.get('batch_size', 'N/A')
        
        row = [
            model_name[:19],
            f"{batch_size}",
            f"{results['best_iou']:.4f}",
            f"{results['best_epoch']}",
            f"{epochs_trained}",
            f"{metrics.get('burned_f1', 0):.4f}",
            f"{metrics.get('burned_precision', 0):.4f}",
            f"{metrics.get('burned_recall', 0):.4f}",
            f"{metrics.get('fps', 0):.1f}"
        ]
        
        row_line = ""
        for value, width in zip(row, col_widths):
            row_line += f"{value:<{width}}"
        print(row_line)
        
        # å¦‚æœä½¿ç”¨äº†æ—©åœï¼Œæ‰“å°é¢å¤–ä¿¡æ¯
        if results.get('early_stopping_stats'):
            stats = results['early_stopping_stats']
            if stats.get('stopped_early'):
                print(f"     â””â”€ æ—©åœäºç¬¬{results['epochs_trained']}è½® (è€å¿ƒç”¨å°½: {stats['patience_used']}æ¬¡éªŒè¯)")
    
    print("-" * sum(col_widths))
    
    # æ€§èƒ½æå‡åˆ†æ
    baseline_iou = results_summary.get('vanilla_resnet18', {}).get('best_iou', 0)
    if baseline_iou > 0:
        print(f"\næ€§èƒ½æå‡åˆ†æï¼ˆç›¸å¯¹äºVanilla ResNet18ï¼‰:")
        print("-" * 50)
        for model_name, results in results_summary.items():
            if model_name != 'vanilla_resnet18':
                improvement = (results['best_iou'] - baseline_iou) / baseline_iou * 100
                print(f"{model_name}: +{improvement:.1f}%")
    
    # è®­ç»ƒæ•ˆç‡ç»Ÿè®¡
    print(f"\nè®­ç»ƒæ•ˆç‡ç»Ÿè®¡:")
    print("-" * 50)
    total_epochs_saved = 0
    max_epochs = ExperimentConfig.BASE_TRAINING['epochs']
    for model_name, results in results_summary.items():
        if results.get('early_stopping_stats'):
            stats = results['early_stopping_stats']
            epochs_saved = max_epochs - results.get('epochs_trained', max_epochs)
            total_epochs_saved += epochs_saved
            if stats.get('stopped_early'):
                print(f"{model_name}: èŠ‚çœäº†{epochs_saved}è½®è®­ç»ƒ")
            else:
                print(f"{model_name}: å®Œæˆå…¨éƒ¨è®­ç»ƒ")
    
    if total_epochs_saved > 0:
        print(f"\næ€»å…±èŠ‚çœè®­ç»ƒè½®æ•°: {total_epochs_saved}")
        print(f"è®­ç»ƒæ•ˆç‡æå‡: {total_epochs_saved/(max_epochs*len(results_summary))*100:.1f}%")
    
    # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š
    report_path = os.path.join(save_dir, f'comparison_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
    
    # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
    def convert_numpy(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, (np.int32, np.int64)):
            return int(obj)
        elif isinstance(obj, dict):
            return {k: convert_numpy(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy(v) for v in obj]
        return obj
    
    with open(report_path, 'w') as f:
        json.dump(convert_numpy(results_summary), f, indent=2)
    
    print(f"\nâœ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    print("="*80)
    
    return report_path


# ==================================
# ä¸»å‡½æ•° - RTX 4090ä¼˜åŒ–ç‰ˆ
# ==================================
def main():
    """è®­ç»ƒæ‰€æœ‰åŸºçº¿æ¨¡å‹å¹¶ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š - RTX 4090ä¼˜åŒ–ç‰ˆ"""
    
    print("="*80)
    print("åŸºçº¿æ¨¡å‹å¯¹æ¯”å®éªŒ - RTX 4090ä¼˜åŒ–ç‰ˆ")
    print("="*80)
    
    # GPUæ£€æµ‹å’Œä¼˜åŒ–
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name()
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"\n æ£€æµ‹åˆ°GPU: {gpu_name}")
        print(f"   æ€»æ˜¾å­˜: {gpu_memory:.1f} GB")
        
        # ä¼˜åŒ–GPUå†…å­˜
        GPUMonitor.optimize_memory()
        GPUMonitor.print_gpu_utilization()
    else:
        print("\n è­¦å‘Š: æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")

    # è·å–åŸºç¡€é…ç½®
    base_config = ExperimentConfig.BASE_TRAINING
    
    # è®¾ç½®éšæœºç§å­
    set_random_seeds(
        seed=base_config['seed'],
        deterministic=base_config['deterministic']
    )
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = base_config.get('save_dir', 'baseline_checkpoints_4090')
    os.makedirs(save_dir, exist_ok=True)
    
    # è®°å½•å®éªŒé…ç½®
    config_path = os.path.join(save_dir, 'experiment_config.json')
    with open(config_path, 'w') as f:
        json.dump({
            'base_config': base_config,
            'model_configs': ExperimentConfig.LOSS_CONFIGS,
            'gpu_info': {
                'device': torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU',
                'memory_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0
            },
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }, f, indent=2)
    print(f"\n å®éªŒé…ç½®å·²ä¿å­˜è‡³: {config_path}")
    
    # éªŒè¯é…ç½®
    print("\néªŒè¯é…ç½®...")
    for model_name in ['vanilla_resnet18', 'resnet18_focal', 'resnet18_skip', 'full_improved']:
        config = ExperimentConfig.get_config(model_name)
        print(f"  {model_name}: batch_size={config['batch_size']}, lr={config.get('scaled_learning_rate', config['learning_rate']):.4f}")
        ExperimentConfig.validate_config(config)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨ä¼˜åŒ–çš„å‚æ•°ï¼‰
    print("\nåŠ è½½æ•°æ®é›†...")
    train_loader, val_loader, train_dataset, val_dataset, dataset_info = create_dataloaders(
    train_dir=base_config['train_dir'],
    test_dir=None,  # Not used for validation, kept for future inference
    batch_size=base_config['batch_size'],
    num_workers=base_config['num_workers'],
    img_size=base_config['img_size'],
    pin_memory=base_config.get('pin_memory', True),
    persistent_workers=base_config.get('persistent_workers', True),
    prefetch_factor=base_config.get('prefetch_factor', 2),
    val_split=base_config.get('val_split', 0.2)  # Add this parameter
)

    # Update the print statements to show more info:
    print(f"è®­ç»ƒé›†: {dataset_info['train_samples']} samples ({len(train_loader)} batches) - with augmentation")
    print(f"éªŒè¯é›†: {dataset_info['val_samples']} samples ({len(val_loader)} batches) - no augmentation")
    print(f"éªŒè¯é›†æ¯”ä¾‹: {dataset_info['val_split']*100:.0f}%\n")
    
    # åˆ›å»ºæ¨¡å‹
    from baseline_models import create_baseline_models
    models = create_baseline_models(pretrained=True, print_info=True)
    
    # è®­ç»ƒç»“æœæ±‡æ€»
    results_summary = {}
    
    # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
    for model_name, model in models.items():
        # è·å–æ¨¡å‹ç‰¹å®šé…ç½®
        config = ExperimentConfig.get_config(model_name)
        config['save_dir'] = save_dir  # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ä¿å­˜ç›®å½•
        
        # é‡æ–°è®¾ç½®éšæœºç§å­ï¼ˆç¡®ä¿æ¯ä¸ªæ¨¡å‹ä»ç›¸åŒçŠ¶æ€å¼€å§‹ï¼‰
        set_random_seeds(seed=config['seed'], deterministic=config['deterministic'])
        
        # ä¸ºä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒçš„batch sizeåˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if config['batch_size'] != base_config['batch_size']:
            print(f"\nä¸º{model_name}åˆ›å»ºç‰¹å®šçš„æ•°æ®åŠ è½½å™¨ (batch_size={config['batch_size']})...")
            model_train_loader, model_val_loader, _, _, _ = create_dataloaders(
                train_dir=config['train_dir'],
                test_dir=None,  # Not used for validation
                batch_size=config['batch_size'],
                num_workers=config['num_workers'],
                img_size=config['img_size'],
                pin_memory=config.get('pin_memory', True),
                persistent_workers=config.get('persistent_workers', True),
                prefetch_factor=config.get('prefetch_factor', 2),
                val_split=config.get('val_split', 0.2)  # Add this parameter
            )
        else:
            model_train_loader = train_loader
            model_val_loader = val_loader
        
        # è®­ç»ƒæ¨¡å‹
        history, best_iou, best_epoch = train_baseline_model(
            model, config, model_train_loader, model_val_loader
        )
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # ä¿å­˜ç»“æœ
        results_summary[model_name] = {
            'best_iou': best_iou,
            'best_epoch': best_epoch,
            'batch_size': config['batch_size'],
            'learning_rate': config.get('scaled_learning_rate', config['learning_rate']),
            'epochs_trained': history['epochs_trained'],
            'early_stopping_stats': history.get('early_stopping_stats'),
            'final_metrics': history['val_metrics'][-1] if history['val_metrics'] else {},
            'history': {
                'train_loss': history['train_loss'],
                'val_loss': history['val_loss'],
                'learning_rates': history['learning_rates']
            }
        }
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = os.path.join(save_dir, f'{model_name}_history.json')
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=2, default=lambda x: float(x) if isinstance(x, (np.float32, np.float64)) else x)
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    report_path = generate_comparison_report(results_summary, save_dir)
    
    # æœ€ç»ˆGPUå†…å­˜ç»Ÿè®¡
    if torch.cuda.is_available():
        print("æœ€ç»ˆGPUå†…å­˜ç»Ÿè®¡:")
        GPUMonitor.print_gpu_utilization()
    
    print("\n" + "="*80)
    print("å®éªŒå®Œæˆï¼")
    print(f"RTX 4090ä¼˜åŒ–é…ç½®æ€»ç»“:")
    print(f"  - åŸºç¡€batch size: {base_config['batch_size']}")
    print(f"  - æ··åˆç²¾åº¦è®­ç»ƒ: {'å¯ç”¨' if base_config.get('use_amp', False) else 'ç¦ç”¨'}")
    print(f"  - å­¦ä¹ ç‡ç¼©æ”¾: {'å¯ç”¨' if base_config.get('scaled_learning_rate') else 'ç¦ç”¨'}")
    print(f"  - æœ€å¤§è®­ç»ƒè½®æ•°: {base_config['epochs']}")
    print(f"  - ä¼˜åŒ–å™¨: {base_config['optimizer']}")
    print(f"  - æ—©åœæœºåˆ¶: {'å¯ç”¨' if base_config['use_early_stopping'] else 'ç¦ç”¨'}")
    if base_config['use_early_stopping']:
        print(f"    - éªŒè¯é¢‘ç‡: æ¯{base_config['validate_every']}è½®")
        print(f"    - è€å¿ƒå€¼: {base_config['early_stopping_patience']}æ¬¡éªŒè¯")
        print(f"    - æœ€å°æ”¹å–„: {base_config['early_stopping_min_delta']}")
    print("å·®å¼‚ä»…åœ¨äºæ¨¡å‹æ¶æ„å’ŒæŸå¤±å‡½æ•°ã€‚")
    print("="*80)


if __name__ == "__main__":
    main()